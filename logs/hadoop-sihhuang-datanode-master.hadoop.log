2015-10-19 00:48:54,425 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 00:48:54,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 00:48:54,796 WARN org.apache.hadoop.hdfs.server.common.Util: Path /root/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 00:48:56,605 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /root/hadoop/hdfs/data : 
java.io.FileNotFoundException: File file:/root/hadoop/hdfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:125)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:142)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1866)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1908)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 00:48:56,611 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/root/hadoop/hdfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1917)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 00:48:56,618 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-19 00:48:56,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/10.64.252.124
************************************************************/
2015-10-19 01:21:27,399 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 01:21:27,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 01:21:27,709 WARN org.apache.hadoop.hdfs.server.common.Util: Path /root/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 01:21:29,603 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /root/hadoop/hdfs/data : 
java.io.FileNotFoundException: File file:/root/hadoop/hdfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:125)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:142)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1866)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1908)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 01:21:29,610 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/root/hadoop/hdfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1917)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 01:21:29,622 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-19 01:21:29,624 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 01:26:01,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 01:26:01,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 01:26:02,236 WARN org.apache.hadoop.hdfs.server.common.Util: Path /root/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 01:26:04,036 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Invalid dfs.datanode.data.dir /root/hadoop/hdfs/data : 
java.io.FileNotFoundException: File file:/root/hadoop/hdfs/data does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:125)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:142)
	at org.apache.hadoop.hdfs.server.datanode.DataNode$DataNodeDiskChecker.checkDir(DataNode.java:1866)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1908)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 01:26:04,043 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
java.io.IOException: All directories in dfs.datanode.data.dir are invalid: "/root/hadoop/hdfs/data" 
	at org.apache.hadoop.hdfs.server.datanode.DataNode.checkStorageLocations(DataNode.java:1917)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:1890)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1782)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:1829)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2005)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2029)
2015-10-19 01:26:04,049 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1
2015-10-19 01:26:04,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 01:39:56,823 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 01:39:56,850 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 01:39:57,493 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 01:40:00,921 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 01:40:01,376 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 01:40:01,376 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 01:40:01,390 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 01:40:01,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 01:40:01,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 01:40:01,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 01:40:01,551 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 01:40:01,809 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 01:40:01,821 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 01:40:01,851 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 01:40:01,858 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 01:40:01,858 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 01:40:01,858 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 01:40:01,890 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 01:40:01,917 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 01:40:01,917 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 01:40:02,706 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 01:40:02,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 01:40:02,991 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 01:40:03,057 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 01:40:03,081 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 01:40:03,120 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 01:40:03,133 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 01:40:03,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 01:40:03,165 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 01:40:03,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-10-19 01:40:03,189 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 01:40:03,190 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 01:40:04,297 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 01:40:04,339 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 5426@master.hadoop
2015-10-19 01:40:04,340 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data is not formatted
2015-10-19 01:40:04,341 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-19 01:40:04,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 01:40:04,967 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 01:40:04,968 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608 is not formatted.
2015-10-19 01:40:04,968 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-19 01:40:04,968 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-882371229-127.0.0.1-1445243950608 directory /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current
2015-10-19 01:40:04,979 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 01:40:04,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=null
2015-10-19 01:40:04,981 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 01:40:05,056 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 01:40:05,064 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 01:40:05,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 01:40:05,178 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445249775178 with interval 21600000
2015-10-19 01:40:05,186 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 01:40:05,194 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 01:40:05,279 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 84ms
2015-10-19 01:40:05,283 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 97ms
2015-10-19 01:40:05,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 01:40:05,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 1ms
2015-10-19 01:40:05,284 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-19 01:40:05,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-10-19 01:40:05,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-10-19 01:40:05,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 01:40:05,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=1
2015-10-19 01:40:05,859 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000
2015-10-19 01:40:06,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 147 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@76f2d3fd
2015-10-19 01:40:06,007 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 01:40:06,013 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 01:40:06,013 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 01:40:06,024 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 01:40:06,024 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 01:40:06,024 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 01:40:06,029 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 02:11:14,526 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 02:11:17,548 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 02:11:17,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 02:13:05,013 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 02:13:05,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 02:13:05,672 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:13:08,330 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 02:13:08,714 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 02:13:08,714 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 02:13:08,718 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 02:13:08,741 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 02:13:08,867 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 02:13:08,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 02:13:08,874 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 02:13:09,184 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 02:13:09,192 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 02:13:09,216 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 02:13:09,224 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 02:13:09,225 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 02:13:09,225 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 02:13:09,272 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 02:13:09,289 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 02:13:09,289 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 02:13:10,486 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 02:13:10,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 02:13:10,914 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 02:13:10,975 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 02:13:11,004 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 02:13:11,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 02:13:11,055 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 02:13:11,080 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 02:13:11,086 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:13:11,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-10-19 02:13:11,096 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 02:13:11,097 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 02:13:11,738 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 02:13:11,767 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 7686@master.hadoop
2015-10-19 02:13:12,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:13:12,239 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 02:13:12,240 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 02:13:12,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 02:13:12,316 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:13:12,326 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 02:13:12,763 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 02:13:12,784 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445257734784 with interval 21600000
2015-10-19 02:13:12,795 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:13:12,806 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:13:12,856 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 24576
2015-10-19 02:13:12,870 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 64ms
2015-10-19 02:13:12,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 75ms
2015-10-19 02:13:12,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:13:12,871 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-19 02:13:12,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-19 02:13:12,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-10-19 02:13:13,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-10-19 02:13:13,078 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 02:13:13,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=8
2015-10-19 02:13:13,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000
2015-10-19 02:13:13,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 135 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5fc0b36d
2015-10-19 02:13:13,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:13:13,479 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 02:13:13,479 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 02:13:13,480 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 02:13:13,480 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 02:13:13,481 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:13:13,489 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 02:16:49,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@72bb89bb
2015-10-19 02:16:49,093 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:18:40,624 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 02:18:40,638 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 02:25:14,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 02:25:14,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 02:25:14,882 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:25:17,717 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 02:25:18,094 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 02:25:18,095 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 02:25:18,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 02:25:18,110 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 02:25:18,240 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 02:25:18,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 02:25:18,248 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 02:25:18,554 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 02:25:18,564 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 02:25:18,590 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 02:25:18,592 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 02:25:18,593 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 02:25:18,593 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 02:25:18,637 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 02:25:18,665 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 02:25:18,665 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 02:25:19,864 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 02:25:20,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 02:25:20,153 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 02:25:20,214 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 02:25:20,241 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 02:25:20,277 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 02:25:20,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 02:25:20,315 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 02:25:20,321 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:25:20,326 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-10-19 02:25:20,333 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 02:25:20,334 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 02:25:21,135 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 02:25:21,177 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 1383@master.hadoop
2015-10-19 02:25:21,653 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:25:21,653 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 02:25:21,662 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 02:25:21,665 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 02:25:21,740 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:25:21,755 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 02:25:22,162 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 02:25:22,191 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445259061191 with interval 21600000
2015-10-19 02:25:22,202 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:25:22,203 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:25:22,253 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 24576
2015-10-19 02:25:22,257 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 54ms
2015-10-19 02:25:22,258 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 55ms
2015-10-19 02:25:22,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:25:22,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 6ms
2015-10-19 02:25:22,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-10-19 02:25:22,268 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-10-19 02:25:22,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-10-19 02:25:22,413 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 02:25:22,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=11
2015-10-19 02:25:22,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000
2015-10-19 02:25:22,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 130 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@26c5f44e
2015-10-19 02:25:22,844 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:25:22,850 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 02:25:22,850 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 02:25:22,851 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 02:25:22,852 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 02:25:22,852 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:25:22,863 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 02:26:25,670 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:512)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 02:26:29,419 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 02:26:30,420 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 02:26:30,477 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 02:26:30,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 02:28:14,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 02:28:14,330 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 02:28:14,971 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:28:17,566 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 02:28:17,945 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 02:28:17,945 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 02:28:17,949 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 02:28:17,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 02:28:18,070 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 02:28:18,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 02:28:18,082 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 02:28:18,384 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 02:28:18,396 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 02:28:18,425 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 02:28:18,428 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 02:28:18,428 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 02:28:18,428 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 02:28:18,472 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 02:28:18,497 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 02:28:18,498 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 02:28:19,709 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 02:28:20,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 02:28:20,117 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 02:28:20,175 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 02:28:20,203 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 02:28:20,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 02:28:20,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 02:28:20,281 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 02:28:20,287 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:28:20,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2015-10-19 02:28:20,297 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 02:28:20,299 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 02:28:20,987 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 02:28:21,028 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 2774@master.hadoop
2015-10-19 02:28:21,526 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:28:21,527 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 02:28:21,527 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 02:28:21,529 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 02:28:21,607 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 02:28:21,630 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 02:28:22,042 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 02:28:22,053 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445258896052 with interval 21600000
2015-10-19 02:28:22,084 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:28:22,085 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:28:22,135 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 24576
2015-10-19 02:28:22,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 64ms
2015-10-19 02:28:22,157 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 73ms
2015-10-19 02:28:22,157 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 02:28:22,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-19 02:28:22,158 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 0ms
2015-10-19 02:28:22,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 beginning handshake with NN
2015-10-19 02:28:22,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to localhost/127.0.0.1:9000 successfully registered with NN
2015-10-19 02:28:22,333 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 02:28:22,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000 trying to claim ACTIVE state with txid=12
2015-10-19 02:28:22,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to localhost/127.0.0.1:9000
2015-10-19 02:28:22,689 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 4 msec to generate and 128 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b381842
2015-10-19 02:28:22,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:28:22,695 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 02:28:22,695 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 02:28:22,697 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 02:28:22,697 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 02:28:22,705 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 02:28:22,712 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 03:14:31,439 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "localhost":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 03:14:33,977 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 03:14:33,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 03:18:02,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 03:18:02,350 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 03:18:02,978 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:18:05,598 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 03:18:05,977 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 03:18:05,977 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 03:18:05,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 03:18:05,998 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 03:18:06,140 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 03:18:06,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 03:18:06,155 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 03:18:06,627 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 03:18:06,631 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 03:18:06,655 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 03:18:06,662 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 03:18:06,665 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 03:18:06,665 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 03:18:06,707 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 03:18:06,733 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 03:18:06,733 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 03:18:07,942 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 03:18:08,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 03:18:08,249 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 03:18:08,307 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 03:18:08,334 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 03:18:08,370 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 03:18:08,383 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 03:18:08,408 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 03:18:08,414 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:18:08,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/127.0.0.1:9000 starting to offer service
2015-10-19 03:18:08,428 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 03:18:08,429 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 03:18:09,192 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 03:18:09,234 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 4523@master.hadoop
2015-10-19 03:18:09,706 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:18:09,707 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 03:18:09,708 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 03:18:09,721 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 03:18:09,785 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:18:09,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 03:18:10,233 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 03:18:10,251 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445252746251 with interval 21600000
2015-10-19 03:18:10,265 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:18:10,271 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 03:18:10,308 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 28672
2015-10-19 03:18:10,336 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 65ms
2015-10-19 03:18:10,344 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 79ms
2015-10-19 03:18:10,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 03:18:10,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-19 03:18:10,349 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 5ms
2015-10-19 03:18:10,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 beginning handshake with NN
2015-10-19 03:18:10,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 successfully registered with NN
2015-10-19 03:18:10,507 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 03:18:10,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 trying to claim ACTIVE state with txid=13
2015-10-19 03:18:10,925 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000
2015-10-19 03:18:11,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 156 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b381842
2015-10-19 03:18:11,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:18:11,095 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 03:18:11,095 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 03:18:11,097 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 03:18:11,097 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 03:18:11,101 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:18:11,120 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 03:26:07,539 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 03:26:11,535 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 03:26:11,599 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 03:26:11,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 03:27:00,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 03:27:00,047 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 03:27:00,662 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:27:03,197 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 03:27:03,569 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 03:27:03,569 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 03:27:03,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 03:27:03,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 03:27:03,690 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 03:27:03,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 03:27:03,694 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 03:27:04,004 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 03:27:04,012 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 03:27:04,036 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 03:27:04,042 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 03:27:04,042 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 03:27:04,042 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 03:27:04,090 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 03:27:04,104 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 03:27:04,104 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 03:27:05,425 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 03:27:06,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 03:27:06,111 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 03:27:06,175 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 03:27:06,202 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 03:27:06,239 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 03:27:06,256 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 03:27:06,280 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 03:27:06,285 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:27:06,295 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/127.0.0.1:9000 starting to offer service
2015-10-19 03:27:06,300 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 03:27:06,308 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 03:27:06,931 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 03:27:06,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 6209@master.hadoop
2015-10-19 03:27:07,242 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:27:07,242 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 03:27:07,243 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 03:27:07,246 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 03:27:07,321 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 03:27:07,344 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 03:27:07,754 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 03:27:07,782 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445250573782 with interval 21600000
2015-10-19 03:27:07,793 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:27:07,794 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 03:27:07,844 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 28672
2015-10-19 03:27:07,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 73ms
2015-10-19 03:27:07,868 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 75ms
2015-10-19 03:27:07,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 03:27:07,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-19 03:27:07,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 13ms
2015-10-19 03:27:07,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 beginning handshake with NN
2015-10-19 03:27:08,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 successfully registered with NN
2015-10-19 03:27:08,039 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 03:27:08,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 trying to claim ACTIVE state with txid=16
2015-10-19 03:27:08,395 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000
2015-10-19 03:27:08,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 118 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@72cdca7b
2015-10-19 03:27:08,514 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:27:08,520 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 03:27:08,520 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 03:27:08,521 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 03:27:08,521 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 03:27:08,522 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 03:27:08,538 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 03:29:33,787 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-882371229-127.0.0.1-1445243950608 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-19 08:41:20,753 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@247cce15
2015-10-19 08:41:20,754 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 09:29:33,784 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-882371229-127.0.0.1-1445243950608 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-19 14:41:21,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7c5f9089
2015-10-19 14:41:21,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 15:29:33,783 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-882371229-127.0.0.1-1445243950608 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-19 18:34:25,358 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 18:34:29,354 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 18:34:29,583 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 18:34:29,589 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 18:42:15,309 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 18:42:15,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 18:42:15,978 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 18:42:18,547 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 18:42:18,941 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 18:42:18,941 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 18:42:18,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 18:42:18,950 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 18:42:19,034 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 18:42:19,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 18:42:19,042 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 18:42:19,363 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 18:42:19,368 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 18:42:19,392 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 18:42:19,399 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 18:42:19,399 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 18:42:19,400 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 18:42:19,448 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 18:42:19,463 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 18:42:19,463 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 18:42:20,691 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 18:42:21,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 18:42:21,190 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 18:42:21,251 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 18:42:21,278 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 18:42:21,317 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 18:42:21,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 18:42:21,356 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 18:42:21,362 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 18:42:21,372 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/127.0.0.1:9000 starting to offer service
2015-10-19 18:42:21,377 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 18:42:21,378 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 18:42:22,029 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 18:42:22,056 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 10178@master.hadoop
2015-10-19 18:42:22,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 18:42:22,510 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 18:42:22,511 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 18:42:22,516 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 18:42:22,584 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 18:42:22,603 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 18:42:23,037 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 18:42:23,049 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445318809049 with interval 21600000
2015-10-19 18:42:23,073 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 18:42:23,074 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 18:42:23,120 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-882371229-127.0.0.1-1445243950608/current: 28672
2015-10-19 18:42:23,140 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 65ms
2015-10-19 18:42:23,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 76ms
2015-10-19 18:42:23,149 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 18:42:23,150 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-19 18:42:23,157 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 8ms
2015-10-19 18:42:23,160 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 beginning handshake with NN
2015-10-19 18:42:23,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 successfully registered with NN
2015-10-19 18:42:23,334 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 18:42:23,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 trying to claim ACTIVE state with txid=49
2015-10-19 18:42:23,612 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000
2015-10-19 18:42:23,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 71 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@12f3aa66
2015-10-19 18:42:23,686 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 18:42:23,692 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 18:42:23,692 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 18:42:23,693 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 18:42:23,693 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 18:42:23,694 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 18:42:23,698 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 18:56:17,592 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:384)
	at org.apache.hadoop.net.SocketInputStream$Reader.performIO(SocketInputStream.java:57)
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:512)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 18:56:21,375 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 18:56:22,376 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 18:56:22,446 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-19 18:56:22,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-19 19:08:54,744 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/127.0.0.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-19 19:08:54,772 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-19 19:08:55,414 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 19:08:58,016 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-19 19:08:58,407 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-19 19:08:58,407 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-19 19:08:58,424 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is localhost
2015-10-19 19:08:58,436 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-19 19:08:58,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-19 19:08:58,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-19 19:08:58,523 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-19 19:08:58,841 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-19 19:08:58,847 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-19 19:08:58,874 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-19 19:08:58,877 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-19 19:08:58,877 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-19 19:08:58,877 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-19 19:08:58,927 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-19 19:08:58,943 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-19 19:08:58,943 INFO org.mortbay.log: jetty-6.1.26
2015-10-19 19:09:00,156 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-19 19:09:00,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-19 19:09:00,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-19 19:09:00,628 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-19 19:09:00,655 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-19 19:09:00,692 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-19 19:09:00,706 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-19 19:09:00,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-19 19:09:00,736 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 19:09:00,746 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/127.0.0.1:9000 starting to offer service
2015-10-19 19:09:00,751 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-19 19:09:00,752 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-19 19:09:01,436 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-19 19:09:01,505 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 13099@master.hadoop
2015-10-19 19:09:02,050 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-882371229-127.0.0.1-1445243950608
2015-10-19 19:09:02,050 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-19 19:09:02,051 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-19 19:09:02,054 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=993055729;bpid=BP-882371229-127.0.0.1-1445243950608;lv=-55;nsInfo=lv=-57;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0;bpid=BP-882371229-127.0.0.1-1445243950608;dnuuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5
2015-10-19 19:09:02,131 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-19 19:09:02,153 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-19 19:09:02,618 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-19 19:09:02,649 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445326987649 with interval 21600000
2015-10-19 19:09:02,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 19:09:02,654 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 19:09:02,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-882371229-127.0.0.1-1445243950608 on /home/sihhuang/hadoop/hdfs/data/current: 68ms
2015-10-19 19:09:02,722 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-882371229-127.0.0.1-1445243950608: 73ms
2015-10-19 19:09:02,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-19 19:09:02,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-882371229-127.0.0.1-1445243950608 on volume /home/sihhuang/hadoop/hdfs/data/current: 1ms
2015-10-19 19:09:02,746 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 24ms
2015-10-19 19:09:02,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 beginning handshake with NN
2015-10-19 19:09:02,856 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid null) service to master.hadoop/127.0.0.1:9000 successfully registered with NN
2015-10-19 19:09:02,857 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/127.0.0.1:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-19 19:09:03,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 trying to claim ACTIVE state with txid=55
2015-10-19 19:09:03,210 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000
2015-10-19 19:09:03,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 7 msec to generate and 129 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@26c5f44e
2015-10-19 19:09:03,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 19:09:03,352 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-19 19:09:03,353 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-19 19:09:03,362 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-19 19:09:03,362 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-19 19:09:03,363 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 19:09:03,367 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-882371229-127.0.0.1-1445243950608 to blockPoolScannerMap, new size=1
2015-10-19 19:32:44,924 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 19:32:48,918 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:49,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:50,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:51,920 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:52,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:53,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:54,922 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:55,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:56,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:57,924 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:57,925 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:32:58,930 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:32:59,931 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:00,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:01,932 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:02,933 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:03,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:04,934 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:05,935 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:06,936 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:07,937 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:07,938 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:08,939 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:09,940 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:10,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:11,941 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:12,942 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:13,943 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:14,944 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:15,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:16,945 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:17,946 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:17,947 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:18,954 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:19,955 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:20,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:21,956 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:22,957 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:23,958 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:24,959 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:25,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:26,960 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:27,961 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:27,962 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:28,964 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:29,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:30,965 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:31,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:32,966 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:33,967 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:34,968 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:35,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:36,969 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:37,970 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:37,971 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:38,973 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:39,974 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:40,975 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:41,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:42,976 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:43,977 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:44,978 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:45,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:46,979 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:47,980 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:47,981 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:48,983 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:49,984 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:50,985 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:51,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:52,986 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:53,987 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:54,988 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:55,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:56,989 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:57,990 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:57,991 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:33:58,993 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:33:59,994 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:00,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:01,995 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:02,996 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:03,997 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:04,998 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:05,999 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:07,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:08,001 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:08,002 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:09,006 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:10,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:11,008 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:12,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:13,009 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:14,010 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:15,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:16,012 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:17,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:18,013 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:18,015 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:19,017 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:20,018 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:21,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:22,019 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:23,020 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:24,021 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:25,022 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:26,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:27,023 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:28,024 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:28,025 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:29,027 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:30,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:31,028 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:32,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:33,029 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:34,030 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:35,032 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:36,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:37,033 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:38,034 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:38,035 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:39,037 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:40,038 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:41,039 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:42,040 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:43,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:44,041 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:45,043 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:46,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:47,044 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:48,045 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:48,046 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:49,048 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:50,049 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:51,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:52,050 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:53,051 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:54,052 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:55,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:56,053 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:57,054 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:58,055 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:34:58,056 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:34:59,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:00,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:01,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:02,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:03,061 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:04,062 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:05,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:06,063 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:07,064 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:08,065 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:08,066 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:09,067 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:10,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:11,069 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:12,070 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:13,071 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:14,072 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:15,073 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:16,074 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:17,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:18,075 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:18,076 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:19,078 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:20,079 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:21,080 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:22,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:23,081 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:24,082 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:25,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:26,084 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:27,085 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:28,086 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:28,088 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:29,089 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:30,090 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:31,091 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:32,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:33,092 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:34,093 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:35,094 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:36,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:37,095 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:38,096 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:38,097 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:39,099 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:40,100 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:41,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:42,101 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:43,102 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:44,103 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:45,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:46,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:47,105 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:48,106 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:48,107 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:49,109 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:50,110 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:51,111 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:52,112 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:53,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:54,113 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:55,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:56,115 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:57,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:58,116 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:35:58,117 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:35:59,119 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:00,120 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:01,121 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:02,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:03,122 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:04,123 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:05,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:06,125 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:07,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:08,126 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:08,127 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:36:09,130 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:10,131 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:11,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:12,132 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:13,133 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:14,134 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:15,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:16,135 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:17,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:18,136 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:18,137 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:36:19,139 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:20,140 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:21,141 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:22,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:23,142 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:24,143 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:25,144 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:26,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:27,149 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:28,150 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:28,151 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:36:29,153 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:30,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:31,154 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:32,155 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:33,156 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:34,157 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:35,158 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:36,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:37,159 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:38,160 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:38,161 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:36:39,178 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:40,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:41,179 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:42,180 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:43,181 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:44,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:45,182 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:46,183 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:47,184 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:48,185 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:36:48,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: DatanodeCommand action : DNA_REGISTER from master.hadoop/127.0.0.1:9000 with active state
2015-10-19 19:36:48,445 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 beginning handshake with NN
2015-10-19 19:36:48,470 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 successfully registered with NN
2015-10-19 19:36:48,486 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 16 msecs for RPC and NN processing.  Got back commands none
2015-10-19 19:37:57,490 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/127.0.0.1"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-19 19:38:01,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:02,495 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:03,496 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:04,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:05,497 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:06,498 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:07,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:08,499 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:09,502 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:10,503 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:10,504 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:38:11,509 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:12,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:13,510 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:14,511 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:15,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:16,512 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:17,513 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:18,514 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:19,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:20,515 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:20,516 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:38:21,518 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:22,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:23,519 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:24,520 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:25,521 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:26,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:27,522 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:28,523 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:29,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:30,524 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:30,525 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:38:31,527 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:32,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:33,528 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:34,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:35,529 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:36,531 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:37,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:38,532 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:39,533 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:40,534 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:40,535 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:38:41,537 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:42,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:43,538 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:44,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:45,539 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:46,540 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:47,541 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:48,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:49,542 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:50,543 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:50,544 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:38:51,546 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:52,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:53,547 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:54,548 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:55,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:56,549 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:57,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:58,550 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:38:59,551 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:00,552 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:00,554 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:39:01,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:02,556 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:03,557 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:04,558 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:05,559 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:06,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:07,560 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:08,561 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:09,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:10,562 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:10,563 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.net.ConnectException: Call From master.hadoop/127.0.0.1 to master.hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor7.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:744)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:606)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:700)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:367)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1463)
	at org.apache.hadoop.ipc.Client.call(Client.java:1382)
	... 14 more
2015-10-19 19:39:11,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:12,565 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:13,566 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:14,567 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:15,568 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:16,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:17,569 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:18,570 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:19,571 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-19 19:39:19,781 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000 is shutting down
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.UnregisteredNodeException): Unregistered server: DatanodeRegistration(127.0.0.1, datanodeUuid=318e4b77-7e28-4c74-a0d3-8505cab7d7e5, infoPort=50075, ipcPort=50020, storageInfo=lv=-55;cid=CID-4ed4c3d6-bc0a-4598-baab-6b1c6fb9f62d;nsid=993055729;c=0)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.verifyRequest(NameNodeRpcServer.java:1131)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat(NameNodeRpcServer.java:1026)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolServerSideTranslatorPB.sendHeartbeat(DatanodeProtocolServerSideTranslatorPB.java:107)
	at org.apache.hadoop.hdfs.protocol.proto.DatanodeProtocolProtos$DatanodeProtocolService$2.callBlockingMethod(DatanodeProtocolProtos.java:28059)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
2015-10-19 19:39:19,782 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5) service to master.hadoop/127.0.0.1:9000
2015-10-19 19:39:19,883 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool BP-882371229-127.0.0.1-1445243950608 (Datanode Uuid 318e4b77-7e28-4c74-a0d3-8505cab7d7e5)
2015-10-19 19:39:19,883 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Removed bpid=BP-882371229-127.0.0.1-1445243950608 from blockPoolScannerMap
2015-10-19 19:39:19,883 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Removing block pool BP-882371229-127.0.0.1-1445243950608
2015-10-19 19:39:21,884 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode
2015-10-19 19:39:21,886 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0
2015-10-19 19:39:21,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/127.0.0.1
************************************************************/
2015-10-22 02:06:38,592 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-22 02:06:38,626 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-22 02:06:39,274 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:06:42,564 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-22 02:06:42,896 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-22 02:06:42,896 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-22 02:06:42,904 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is master.hadoop
2015-10-22 02:06:42,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-22 02:06:43,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-22 02:06:43,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-22 02:06:43,058 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-22 02:06:43,297 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-22 02:06:43,301 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-22 02:06:43,326 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-22 02:06:43,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-22 02:06:43,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-22 02:06:43,337 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-22 02:06:43,369 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-22 02:06:43,388 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-22 02:06:43,388 INFO org.mortbay.log: jetty-6.1.26
2015-10-22 02:06:44,280 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-22 02:06:44,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-22 02:06:44,567 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-22 02:06:44,635 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-22 02:06:44,659 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-22 02:06:44,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-22 02:06:44,709 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-22 02:06:44,769 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-22 02:06:44,779 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:06:44,783 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/10.64.252.124:9000 starting to offer service
2015-10-22 02:06:44,797 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-22 02:06:44,798 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-22 02:06:45,818 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-22 02:06:45,846 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 8036@master.hadoop
2015-10-22 02:06:45,847 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data is not formatted
2015-10-22 02:06:45,847 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-22 02:06:46,501 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:06:46,501 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-22 02:06:46,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data/current/BP-2016584841-127.0.0.1-1445320775798 is not formatted.
2015-10-22 02:06:46,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-22 02:06:46,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-2016584841-127.0.0.1-1445320775798 directory /home/sihhuang/hadoop/hdfs/data/current/BP-2016584841-127.0.0.1-1445320775798/current
2015-10-22 02:06:46,512 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-22 02:06:46,515 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=681565167;bpid=BP-2016584841-127.0.0.1-1445320775798;lv=-55;nsInfo=lv=-57;cid=CID-53e1715d-0f2f-4241-8a3c-8c1633b6b4f6;nsid=681565167;c=0;bpid=BP-2016584841-127.0.0.1-1445320775798;dnuuid=null
2015-10-22 02:06:46,520 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 6743702c-dd0f-4948-b82e-c978a9b8bf5d
2015-10-22 02:06:46,575 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:06:46,586 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-22 02:06:46,633 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-22 02:06:46,641 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445515704640 with interval 21600000
2015-10-22 02:06:46,650 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:06:46,651 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:06:46,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-2016584841-127.0.0.1-1445320775798 on /home/sihhuang/hadoop/hdfs/data/current: 108ms
2015-10-22 02:06:46,760 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-2016584841-127.0.0.1-1445320775798: 110ms
2015-10-22 02:06:46,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:06:46,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-22 02:06:46,762 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 2ms
2015-10-22 02:06:46,768 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 beginning handshake with NN
2015-10-22 02:06:46,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 successfully registered with NN
2015-10-22 02:06:46,940 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/10.64.252.124:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-22 02:06:47,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid 6743702c-dd0f-4948-b82e-c978a9b8bf5d) service to master.hadoop/10.64.252.124:9000 trying to claim ACTIVE state with txid=114
2015-10-22 02:06:47,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid 6743702c-dd0f-4948-b82e-c978a9b8bf5d) service to master.hadoop/10.64.252.124:9000
2015-10-22 02:06:47,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 158 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@34f5d30c
2015-10-22 02:06:47,414 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:06:47,420 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-22 02:06:47,420 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-22 02:06:47,425 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-22 02:06:47,425 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-22 02:06:47,426 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:06:47,430 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-2016584841-127.0.0.1-1445320775798 to blockPoolScannerMap, new size=1
2015-10-22 02:30:05,004 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/10.64.252.124"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-22 02:30:09,000 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/10.64.252.124:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-22 02:30:09,107 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-22 02:30:09,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/10.64.252.124
************************************************************/
2015-10-22 02:31:25,412 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-22 02:31:25,441 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-22 02:31:26,060 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:31:28,679 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-22 02:31:29,059 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-22 02:31:29,059 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-22 02:31:29,066 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is master.hadoop
2015-10-22 02:31:29,068 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-22 02:31:29,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-22 02:31:29,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-22 02:31:29,227 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-22 02:31:29,657 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-22 02:31:29,665 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-22 02:31:29,685 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-22 02:31:29,692 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-22 02:31:29,692 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-22 02:31:29,692 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-22 02:31:29,741 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-22 02:31:29,760 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-22 02:31:29,760 INFO org.mortbay.log: jetty-6.1.26
2015-10-22 02:31:30,967 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-22 02:31:31,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-22 02:31:31,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-22 02:31:31,351 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-22 02:31:31,380 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-22 02:31:31,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-22 02:31:31,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-22 02:31:31,463 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-22 02:31:31,469 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:31:31,479 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/10.64.252.124:9000 starting to offer service
2015-10-22 02:31:31,484 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-22 02:31:31,485 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-22 02:31:32,318 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-22 02:31:32,350 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 9630@master.hadoop
2015-10-22 02:31:32,827 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:31:32,827 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-22 02:31:32,828 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-22 02:31:32,841 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=681565167;bpid=BP-2016584841-127.0.0.1-1445320775798;lv=-55;nsInfo=lv=-57;cid=CID-53e1715d-0f2f-4241-8a3c-8c1633b6b4f6;nsid=681565167;c=0;bpid=BP-2016584841-127.0.0.1-1445320775798;dnuuid=6743702c-dd0f-4948-b82e-c978a9b8bf5d
2015-10-22 02:31:32,912 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:31:32,929 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-22 02:31:33,392 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-22 02:31:33,410 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445507068410 with interval 21600000
2015-10-22 02:31:33,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:31:33,414 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:31:33,450 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-2016584841-127.0.0.1-1445320775798/current: 24576
2015-10-22 02:31:33,456 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-2016584841-127.0.0.1-1445320775798 on /home/sihhuang/hadoop/hdfs/data/current: 35ms
2015-10-22 02:31:33,461 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-2016584841-127.0.0.1-1445320775798: 48ms
2015-10-22 02:31:33,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:31:33,462 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-2016584841-127.0.0.1-1445320775798 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-22 02:31:33,465 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-22 02:31:33,471 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 beginning handshake with NN
2015-10-22 02:31:33,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 successfully registered with NN
2015-10-22 02:31:33,637 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/10.64.252.124:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-22 02:31:34,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid 6743702c-dd0f-4948-b82e-c978a9b8bf5d) service to master.hadoop/10.64.252.124:9000 trying to claim ACTIVE state with txid=117
2015-10-22 02:31:34,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-2016584841-127.0.0.1-1445320775798 (Datanode Uuid 6743702c-dd0f-4948-b82e-c978a9b8bf5d) service to master.hadoop/10.64.252.124:9000
2015-10-22 02:31:34,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 7 msec to generate and 176 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b381842
2015-10-22 02:31:34,253 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:31:34,264 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-22 02:31:34,264 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-22 02:31:34,271 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-22 02:31:34,272 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-22 02:31:34,272 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-2016584841-127.0.0.1-1445320775798
2015-10-22 02:31:34,281 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-2016584841-127.0.0.1-1445320775798 to blockPoolScannerMap, new size=1
2015-10-22 02:44:28,414 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-2016584841-127.0.0.1-1445320775798 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-22 02:49:36,695 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/10.64.252.124"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-22 02:49:40,691 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master.hadoop/10.64.252.124:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-10-22 02:49:40,817 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-22 02:49:40,820 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/10.64.252.124
************************************************************/
2015-10-22 02:52:38,735 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-22 02:52:38,765 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-22 02:52:39,415 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:52:42,908 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-22 02:52:43,281 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-22 02:52:43,281 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-22 02:52:43,285 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is master.hadoop
2015-10-22 02:52:43,291 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-22 02:52:43,420 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-22 02:52:43,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-22 02:52:43,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-22 02:52:43,682 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-22 02:52:43,694 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-22 02:52:43,721 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-22 02:52:43,724 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-22 02:52:43,724 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-22 02:52:43,725 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-22 02:52:43,762 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-22 02:52:43,784 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-22 02:52:43,784 INFO org.mortbay.log: jetty-6.1.26
2015-10-22 02:52:44,939 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-22 02:52:45,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-22 02:52:45,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-22 02:52:45,294 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-22 02:52:45,317 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-22 02:52:45,355 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-22 02:52:45,369 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-22 02:52:45,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-22 02:52:45,398 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:52:45,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/10.64.252.124:9000 starting to offer service
2015-10-22 02:52:45,409 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-22 02:52:45,410 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-22 02:52:45,915 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-22 02:52:45,949 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 11165@master.hadoop
2015-10-22 02:52:45,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data is not formatted
2015-10-22 02:52:45,950 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-22 02:52:46,576 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1793050028-10.64.252.124-1445507533643
2015-10-22 02:52:46,576 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-22 02:52:46,597 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /home/sihhuang/hadoop/hdfs/data/current/BP-1793050028-10.64.252.124-1445507533643 is not formatted.
2015-10-22 02:52:46,597 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting ...
2015-10-22 02:52:46,597 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1793050028-10.64.252.124-1445507533643 directory /home/sihhuang/hadoop/hdfs/data/current/BP-1793050028-10.64.252.124-1445507533643/current
2015-10-22 02:52:46,606 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-22 02:52:46,607 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1240493319;bpid=BP-1793050028-10.64.252.124-1445507533643;lv=-55;nsInfo=lv=-57;cid=CID-f4c5d5b1-ac79-4290-9a6c-377b5c7521eb;nsid=1240493319;c=0;bpid=BP-1793050028-10.64.252.124-1445507533643;dnuuid=null
2015-10-22 02:52:46,613 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 7ed920a3-71df-4833-8842-f7d8c5e530b6
2015-10-22 02:52:46,676 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-22 02:52:46,695 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-22 02:52:46,774 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-22 02:52:46,789 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445521509789 with interval 21600000
2015-10-22 02:52:46,817 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 02:52:46,818 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:52:46,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1793050028-10.64.252.124-1445507533643 on /home/sihhuang/hadoop/hdfs/data/current: 57ms
2015-10-22 02:52:46,876 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1793050028-10.64.252.124-1445507533643: 58ms
2015-10-22 02:52:46,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-22 02:52:46,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-22 02:52:46,881 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 6ms
2015-10-22 02:52:46,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 beginning handshake with NN
2015-10-22 02:52:46,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 successfully registered with NN
2015-10-22 02:52:46,962 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/10.64.252.124:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-22 02:52:47,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000 trying to claim ACTIVE state with txid=1
2015-10-22 02:52:47,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000
2015-10-22 02:52:47,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 97 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@53a9dd25
2015-10-22 02:52:47,351 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 02:52:47,357 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-22 02:52:47,357 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-22 02:52:47,358 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-22 02:52:47,358 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-22 02:52:47,359 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 02:52:47,364 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1793050028-10.64.252.124-1445507533643 to blockPoolScannerMap, new size=1
2015-10-22 05:10:17,292 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@63eea5be
2015-10-22 05:10:17,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 06:45:09,813 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-22 11:10:18,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@19d158b1
2015-10-22 11:10:18,258 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 12:45:09,810 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-22 17:10:16,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@55e73e4b
2015-10-22 17:10:16,243 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-22 18:45:09,810 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-22 23:10:17,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@7f3f6b4f
2015-10-22 23:10:17,186 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 00:45:09,809 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-23 02:24:50,644 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/10.64.252.124"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-23 02:24:53,725 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-23 02:24:53,731 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/10.64.252.124
************************************************************/
2015-10-23 02:25:48,232 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-23 02:25:48,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-23 02:25:48,902 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:25:51,613 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-23 02:25:52,169 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-23 02:25:52,169 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-23 02:25:52,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is master.hadoop
2015-10-23 02:25:52,179 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-23 02:25:52,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-23 02:25:52,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-23 02:25:52,304 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-23 02:25:52,637 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-23 02:25:52,642 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-23 02:25:52,668 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-23 02:25:52,675 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-23 02:25:52,675 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-23 02:25:52,675 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-23 02:25:52,719 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-23 02:25:52,736 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-23 02:25:52,736 INFO org.mortbay.log: jetty-6.1.26
2015-10-23 02:25:54,237 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-23 02:25:54,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-23 02:25:54,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-23 02:25:54,800 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-23 02:25:54,830 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-23 02:25:54,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-23 02:25:54,882 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-23 02:25:54,908 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-23 02:25:54,915 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:25:54,945 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/10.64.252.124:9000 starting to offer service
2015-10-23 02:25:54,949 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-23 02:25:54,951 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-23 02:25:55,335 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-23 02:25:55,356 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 17413@master.hadoop
2015-10-23 02:25:55,778 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:25:55,779 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-23 02:25:55,780 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-23 02:25:55,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1240493319;bpid=BP-1793050028-10.64.252.124-1445507533643;lv=-55;nsInfo=lv=-57;cid=CID-f4c5d5b1-ac79-4290-9a6c-377b5c7521eb;nsid=1240493319;c=0;bpid=BP-1793050028-10.64.252.124-1445507533643;dnuuid=7ed920a3-71df-4833-8842-f7d8c5e530b6
2015-10-23 02:25:55,864 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:25:55,886 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-23 02:25:56,405 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-23 02:25:56,426 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445604634426 with interval 21600000
2015-10-23 02:25:56,429 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:25:56,437 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-23 02:25:56,487 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-1793050028-10.64.252.124-1445507533643/current: 24576
2015-10-23 02:25:56,510 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1793050028-10.64.252.124-1445507533643 on /home/sihhuang/hadoop/hdfs/data/current: 72ms
2015-10-23 02:25:56,517 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1793050028-10.64.252.124-1445507533643: 88ms
2015-10-23 02:25:56,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-23 02:25:56,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-23 02:25:56,521 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 4ms
2015-10-23 02:25:56,524 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 beginning handshake with NN
2015-10-23 02:25:56,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 successfully registered with NN
2015-10-23 02:25:56,599 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/10.64.252.124:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-23 02:25:56,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000 trying to claim ACTIVE state with txid=51
2015-10-23 02:25:56,913 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000
2015-10-23 02:25:57,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 89 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b381842
2015-10-23 02:25:57,009 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:25:57,017 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-23 02:25:57,017 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-23 02:25:57,018 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-23 02:25:57,018 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-23 02:25:57,019 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:25:57,030 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1793050028-10.64.252.124-1445507533643 to blockPoolScannerMap, new size=1
2015-10-23 02:26:20,604 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.IOException: Failed on local exception: java.io.EOFException; Host Details : local host is: "master.hadoop/10.64.252.124"; destination host is: "master.hadoop":9000; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)
	at org.apache.hadoop.ipc.Client.call(Client.java:1415)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy11.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:178)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:566)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:664)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:834)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1055)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:950)
2015-10-23 02:26:23,216 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2015-10-23 02:26:23,219 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at master.hadoop/10.64.252.124
************************************************************/
2015-10-23 02:29:34,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   host = master.hadoop/10.64.252.124
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.5.2
STARTUP_MSG:   classpath = /usr/software/hadoop-2.5.2/etc/hadoop:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/slf4j-api-1.7.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-net-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-compiler-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hadoop-auth-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/common/hadoop-common-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jsp-api-2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-el-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jasper-runtime-5.5.23.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-nfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/hdfs/hadoop-hdfs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jsr305-1.3.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jline-0.9.94.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/activation-1.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-collections-3.2.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-client-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-api-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-server-tests-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/yarn/hadoop-yarn-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hadoop-annotations-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2-tests.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.5.2.jar:/usr/software/hadoop-2.5.2/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.5.2.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r cc72e9b000545b86b75a61f4835eb86d57bfafc0; compiled by 'jenkins' on 2014-11-14T23:45Z
STARTUP_MSG:   java = 1.7.0_80
************************************************************/
2015-10-23 02:29:34,305 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2015-10-23 02:29:34,924 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:29:37,626 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2015-10-23 02:29:38,021 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2015-10-23 02:29:38,021 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2015-10-23 02:29:38,040 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is master.hadoop
2015-10-23 02:29:38,053 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2015-10-23 02:29:38,161 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2015-10-23 02:29:38,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 1048576 bytes/s
2015-10-23 02:29:38,169 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 5
2015-10-23 02:29:38,602 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2015-10-23 02:29:38,612 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2015-10-23 02:29:38,650 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2015-10-23 02:29:38,655 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2015-10-23 02:29:38,655 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2015-10-23 02:29:38,655 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2015-10-23 02:29:38,718 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.datanode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2015-10-23 02:29:38,736 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50075
2015-10-23 02:29:38,736 INFO org.mortbay.log: jetty-6.1.26
2015-10-23 02:29:39,937 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50075
2015-10-23 02:29:40,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = sihhuang
2015-10-23 02:29:40,512 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2015-10-23 02:29:40,572 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2015-10-23 02:29:40,600 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2015-10-23 02:29:40,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2015-10-23 02:29:40,656 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2015-10-23 02:29:40,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2015-10-23 02:29:40,688 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:29:40,697 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to master.hadoop/10.64.252.124:9000 starting to offer service
2015-10-23 02:29:40,701 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2015-10-23 02:29:40,702 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2015-10-23 02:29:41,178 INFO org.apache.hadoop.hdfs.server.common.Storage: Data-node version: -55 and name-node layout version: -57
2015-10-23 02:29:41,198 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /home/sihhuang/hadoop/hdfs/data/in_use.lock acquired by nodename 18329@master.hadoop
2015-10-23 02:29:41,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:29:41,464 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled
2015-10-23 02:29:41,466 INFO org.apache.hadoop.hdfs.server.common.Storage: Restored 0 block files from trash.
2015-10-23 02:29:41,467 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1240493319;bpid=BP-1793050028-10.64.252.124-1445507533643;lv=-55;nsInfo=lv=-57;cid=CID-f4c5d5b1-ac79-4290-9a6c-377b5c7521eb;nsid=1240493319;c=0;bpid=BP-1793050028-10.64.252.124-1445507533643;dnuuid=7ed920a3-71df-4833-8842-f7d8c5e530b6
2015-10-23 02:29:41,526 WARN org.apache.hadoop.hdfs.server.common.Util: Path /home/sihhuang/hadoop/hdfs/data should be specified as a URI in configuration files. Please update hdfs configuration.
2015-10-23 02:29:41,532 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /home/sihhuang/hadoop/hdfs/data/current, StorageType: DISK
2015-10-23 02:29:41,993 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2015-10-23 02:29:42,015 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 1445608164015 with interval 21600000
2015-10-23 02:29:42,024 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:29:42,025 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-23 02:29:42,087 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Cached dfsUsed found for /home/sihhuang/hadoop/hdfs/data/current/BP-1793050028-10.64.252.124-1445507533643/current: 24576
2015-10-23 02:29:42,101 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1793050028-10.64.252.124-1445507533643 on /home/sihhuang/hadoop/hdfs/data/current: 76ms
2015-10-23 02:29:42,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1793050028-10.64.252.124-1445507533643: 85ms
2015-10-23 02:29:42,109 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current...
2015-10-23 02:29:42,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1793050028-10.64.252.124-1445507533643 on volume /home/sihhuang/hadoop/hdfs/data/current: 0ms
2015-10-23 02:29:42,110 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 1ms
2015-10-23 02:29:42,113 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 beginning handshake with NN
2015-10-23 02:29:42,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid null) service to master.hadoop/10.64.252.124:9000 successfully registered with NN
2015-10-23 02:29:42,185 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode master.hadoop/10.64.252.124:9000 using DELETEREPORT_INTERVAL of 300000 msec  BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2015-10-23 02:29:42,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000 trying to claim ACTIVE state with txid=52
2015-10-23 02:29:42,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode Block pool BP-1793050028-10.64.252.124-1445507533643 (Datanode Uuid 7ed920a3-71df-4833-8842-f7d8c5e530b6) service to master.hadoop/10.64.252.124:9000
2015-10-23 02:29:42,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 2 msec to generate and 128 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@3b381842
2015-10-23 02:29:42,619 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:29:42,631 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlockMap
2015-10-23 02:29:42,631 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2015-10-23 02:29:42,632 INFO org.apache.hadoop.util.GSet: 0.5% max memory 966.7 MB = 4.8 MB
2015-10-23 02:29:42,632 INFO org.apache.hadoop.util.GSet: capacity      = 2^19 = 524288 entries
2015-10-23 02:29:42,633 INFO org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner: Periodic Block Verification Scanner initialized with interval 504 hours for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 02:29:42,643 INFO org.apache.hadoop.hdfs.server.datanode.DataBlockScanner: Added bpid=BP-1793050028-10.64.252.124-1445507533643 to blockPoolScannerMap, new size=1
2015-10-23 04:18:54,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1c121308
2015-10-23 04:18:54,462 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 06:49:24,019 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-23 10:18:55,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@72f51247
2015-10-23 10:18:55,331 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 12:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-23 16:18:56,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 1 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5286fcc4
2015-10-23 16:18:56,177 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-23 18:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-23 22:18:56,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@5d035b66
2015-10-23 22:18:56,965 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-24 00:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-24 04:18:54,759 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@1480606f
2015-10-24 04:18:54,760 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-24 06:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-24 10:18:55,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@169b36f5
2015-10-24 10:18:55,577 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-24 12:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-24 16:18:56,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@666ee19c
2015-10-24 16:18:56,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-24 18:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-24 22:18:54,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@66878034
2015-10-24 22:18:54,406 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-25 00:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-25 04:18:55,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@260b6b83
2015-10-25 04:18:55,324 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-25 06:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-25 10:18:56,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 2 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@4522b334
2015-10-25 10:18:56,167 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-25 12:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-25 16:18:56,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@72330de8
2015-10-25 16:18:56,988 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-25 18:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2015-10-25 22:18:54,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Sent 1 blockreports 0 blocks total. Took 0 msec to generate and 1 msecs for RPC and NN processing.  Got back commands org.apache.hadoop.hdfs.server.protocol.FinalizeCommand@722d9a02
2015-10-25 22:18:54,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1793050028-10.64.252.124-1445507533643
2015-10-26 00:49:24,016 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1793050028-10.64.252.124-1445507533643 Total blocks: 0, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
